{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set up PySpark Session\n",
    "from pyspark.sql import SparkSession\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataSkewHandling\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a Sample DataFrame with Skew\n",
    "from pyspark.sql import functions as F\n",
    "# Create a DataFrame with skewed data\n",
    "data = [(1, \"A\")] * 1000 + [(2, \"B\")] * 100 + [(3, \"C\")] * 10\n",
    "df = spark.createDataFrame(data, [\"id\", \"category\"])\n",
    "# Show the DataFrame\n",
    "print(\"Sample DataFrame:\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Diagnose Data Skew\n",
    "# Check the number of rows per partition\n",
    "print(\"\\nNumber of rows per partition:\")\n",
    "df.groupBy(F.spark_partition_id()).count().show()\n",
    "# Inspect data distribution in partitions\n",
    "print(\"\\nData in partitions (first 2 rows per partition):\")\n",
    "partitions = df.rdd.glom().collect()\n",
    "for i, partition in enumerate(partitions):\n",
    "    print(f\"Partition {i}: {partition[:2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of 'id' across partitions\n",
    "df.withColumn(\"partition_id\", F.spark_partition_id()) \\\n",
    "  .groupBy(\"partition_id\", \"id\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"partition_id\", \"id\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Handle Data Skew - Repartition by Column\n",
    "# Repartition the DataFrame by the skewed column\n",
    "print(\"\\nRepartitioning by 'id' column...\")\n",
    "df_repartitioned = df.repartition(\"id\")\n",
    "# Check the new distribution\n",
    "print(\"\\nNumber of rows per partition after repartitioning:\")\n",
    "df_repartitioned.groupBy(F.spark_partition_id()).count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Handle Data Skew - Salting\n",
    "# Add a salt column to evenly distribute data\n",
    "print(\"\\nAdding a salt column for even distribution...\")\n",
    "df_salted = df.withColumn(\"salt\", F.rand())\n",
    "# Repartition by the salt column\n",
    "df_salted = df_salted.repartition(8, \"salt\")\n",
    "# Check the new distribution\n",
    "print(\"\\nNumber of rows per partition after salting:\")\n",
    "df_salted.groupBy(F.spark_partition_id()).count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
