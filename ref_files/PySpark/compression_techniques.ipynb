{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestCompression:\n",
    "    \"\"\"\n",
    "    Class to test compression and decompression benchmarks in Apache Spark.\n",
    "    \"\"\"\n",
    "    file_name: str\n",
    "    df: \"pyspark.sql.DataFrame\"\n",
    "\n",
    "    @staticmethod\n",
    "    def measure_execution_time(func):\n",
    "        \"\"\"Decorator to measure function execution time.\"\"\"\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            start_time = time.time()\n",
    "            result = func(*args, **kwargs)\n",
    "            end_time = time.time()\n",
    "            execution_time = end_time - start_time\n",
    "            print(f\"{func.__name__}: {execution_time:.4f} seconds\")\n",
    "            return result, execution_time\n",
    "        return wrapper\n",
    "\n",
    "    @measure_execution_time\n",
    "    def test_compression(self, format: str, compression: str) -> tuple:\n",
    "        \"\"\"\n",
    "        Test compression by writing a DataFrame to disk.\n",
    "        \"\"\"\n",
    "        output_path = f\"output/{format}_{compression}\"\n",
    "\n",
    "        # Ensure output directory is empty before writing\n",
    "        if os.path.exists(output_path):\n",
    "            for file in os.listdir(output_path):\n",
    "                os.remove(os.path.join(output_path, file))\n",
    "\n",
    "        # Write DataFrame\n",
    "        self.df.write.mode(\"overwrite\").format(format).option(\"compression\", compression).save(output_path)\n",
    "\n",
    "        # Ensure directory exists before calculating size\n",
    "        file_size_mb = 0.0\n",
    "        if os.path.exists(output_path):\n",
    "            file_size = sum(\n",
    "                os.path.getsize(os.path.join(output_path, f))\n",
    "                for f in os.listdir(output_path) if os.path.isfile(os.path.join(output_path, f))\n",
    "            )\n",
    "            file_size_mb = round(file_size / (1024 * 1024), 2)  # Convert to MB\n",
    "\n",
    "        return file_size_mb\n",
    "\n",
    "    @measure_execution_time\n",
    "    def test_decompression(self, format: str, compression: str) -> None:\n",
    "        \"\"\"\n",
    "        Test decompression by reading a DataFrame from disk.\n",
    "        \"\"\"\n",
    "        input_path = f\"output/{format}_{compression}\"\n",
    "        _ = spark.read.format(format).load(input_path).count()\n",
    "\n",
    "    def test_compression_benchmarks(self):\n",
    "        \"\"\"\n",
    "        Run compression and decompression benchmarks for Parquet using Zstd, Snappy, and LZ4.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        codecs = [\"zstd\", \"snappy\", \"lz4\"]\n",
    "\n",
    "        for codec in codecs:\n",
    "            # Test compression\n",
    "            size, compression_time = self.test_compression(\"parquet\", codec)\n",
    "            \n",
    "            # Test decompression\n",
    "            _, decompression_time = self.test_decompression(\"parquet\", codec)\n",
    "            \n",
    "            results[f\"parquet_{codec}\"] = {\n",
    "                \"size_mb\": size,\n",
    "                \"compression_time_seconds\": compression_time,\n",
    "                \"decompression_time_seconds\": decompression_time\n",
    "            }\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_original_file_size(self) -> float:\n",
    "        \"\"\"\n",
    "        Get the size of the original CSV file.\n",
    "        \"\"\"\n",
    "        if os.path.exists(self.file_name):\n",
    "            file_size = os.path.getsize(self.file_name)\n",
    "            return round(file_size / (1024 * 1024), 2)  # Convert to MB\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.appName(\"CompressionBenchmark\").getOrCreate()\n",
    "\n",
    "    # Path to CSV file\n",
    "    FILE_PATH = \"/workspaces/high-performance-pyspark-advanced-strategies-for-optimal-data-processing-3919191/data/test_data.csv\"\n",
    "\n",
    "    # Read CSV into DataFrame with correct data types\n",
    "    df = spark.read.option(\"header\", \"true\").csv(FILE_PATH)\n",
    "    \n",
    "    # Ensure DataFrame is not empty\n",
    "    if df.count() == 0:\n",
    "        print(\"‚ùå ERROR: DataFrame is empty. Please check the input CSV file.\")\n",
    "        spark.stop()\n",
    "        exit()\n",
    "\n",
    "    # Get original file size before compression\n",
    "    test_compression = TestCompression(file_name=FILE_PATH, df=df)\n",
    "    original_size_mb = test_compression.get_original_file_size()\n",
    "\n",
    "    # Run compression and decompression benchmarks\n",
    "    results = test_compression.test_compression_benchmarks()\n",
    "\n",
    "    # Print original file size and benchmark results\n",
    "    print(f\"\\nOriginal CSV file size: {original_size_mb} MB\\n\")\n",
    "    print(\"üî• Compression and Decompression Benchmark Results üî• \\n\")\n",
    "    for algorithm, metrics in results.items():\n",
    "        print(f\"{algorithm}: \\n \")\n",
    "        print(f\"  Compression Time: {metrics['compression_time_seconds']:.2f}s\")\n",
    "        print(f\"  Decompression Time: {metrics['decompression_time_seconds']:.2f}s\")\n",
    "        print(f\"  Size: {metrics['size_mb']} MB\\n\")\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
