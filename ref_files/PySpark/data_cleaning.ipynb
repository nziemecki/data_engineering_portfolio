{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, isnull, count, regexp_extract, split, coalesce, lit, to_date\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, ArrayType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"HighPerformancePySpark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to WARN to reduce verbosity\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for the dataset\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_details\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"quantity\", StringType(), True),\n",
    "    StructField(\"price_per_unit\", StringType(), True),\n",
    "    StructField(\"tags\", StringType(), True),\n",
    "    StructField(\"items\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.csv(\"/workspaces/high-performance-pyspark-advanced-strategies-for-optimal-data-processing-3919191/data/online_sales_data.csv\", schema=schema, header=True)\n",
    "\n",
    "# Display the dataset\n",
    "print(\"Raw Dataset:\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the NULLs in the dataframe\n",
    "\n",
    "null_values_count = df.select([count(when(isnull(c) , c)).alias(c) for c in df.columns])\n",
    "\n",
    "null_values_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect rows with negative quantity or invalid price\n",
    "df_invalid = df.filter((col(\"quantity\") < 0) | (col(\"quantity\").rlike(\"^[^0-9]\")) | (col(\"price_per_unit\").rlike(\"^[^0-9]\")))\n",
    "df_invalid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is skewed in qunatity column. So we will impute median for NULLs in this column\n",
    "# \"quantity\" is String type, first we will cast it to Numeric and then impute the Median\n",
    "df = df.withColumn(\"quantity\" , col(\"quantity\").cast(\"double\"))\n",
    "# Step 1: Calculate the median of the 'quantity' column\n",
    "median_quantity = df.approxQuantile(\"quantity\", [0.5], 0.0)[0]  # 0.5 for median (50th percentile)\n",
    "df = df.withColumn(\"quantity\" , when(col(\"quantity\").isNull() , median_quantity).otherwise(col(\"quantity\")))\n",
    "df = df.withColumn(\"price_per_unit\" , when(col(\"price_per_unit\") == \"fifty\", 50.00 ).otherwise(col(\"price_per_unit\")))\n",
    "df = df.withColumn(\"quantity\" , when(col(\"quantity\") < 0 , 0).otherwise(col(\"quantity\")))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Customer Name\", split(col(\"customer_details\") , \"\\\\|\")[0]).\\\n",
    "         withColumn(\"Customer Address\" , split(col(\"customer_details\"), \"\\\\|\")[1])\n",
    "\n",
    "df = df.withColumn(\"Customer Address\" , when(col(\"Customer Address\").isNull() , lit(\"Unknown\")).otherwise(col(\"Customer Address\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "# Example of extracting components from customer_details (e.g., street name, city)\n",
    "df_address_split = df.withColumn('street', regexp_extract(col('customer_details'), r'(\\d+ Street Name)', 1)) \\\n",
    "                     .withColumn('city', regexp_extract(col('customer_details'), r'City (\\d+)', 1))\n",
    "df_address_split.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is skewed in quantity column. \n",
    "# So we will impute median for NULLs in this column\n",
    "# \"quantity\" is String type, first we will cast it to Numeric \n",
    "# Then impute the Median\n",
    "df = df.withColumn(\"quantity\" , col(\"quantity\").cast(\"double\"))\n",
    "\n",
    "# Step 1: Calculate the median of the 'quantity' column\n",
    "\n",
    "median_quantity = df.approxQuantile(\"quantity\", [0.5], 0.0)[0]  # 0.5 for median (50th percentile)\n",
    "df = df.withColumn(\"quantity\" , when(col(\"quantity\").isNull() , median_quantity).otherwise(col(\"quantity\")))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
